{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PersonalityPrediction-myPersonality.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "12qYtNnXCwtfAoGFwxct5w-25kdhMSOEQ",
      "authorship_tag": "ABX9TyNbNJCJd7H0tCSR4wvV9tds",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yudhiesh/PyTorch/blob/master/PersonalityPrediction_myPersonality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAULGvQzII3J",
        "outputId": "7f634a97-39c1-4925-a5c8-ffe4c2ffa916"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb 21 18:47:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzQxE1VyIaUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9ea017-ec23-4632-c913-29d62d83457d"
      },
      "source": [
        "!pip install transformers==3\n",
        "!pip install pytorch-lightning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.4.1)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.8.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.25.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (53.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch-lightning) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (3.7.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2020.12.5)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (5.1.0)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSK5WV4eIpOi"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix, classification_report\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional.classification import auroc\n",
        "import re\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gkp_IiUoJY4"
      },
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TNu31zdI5TR"
      },
      "source": [
        "PATH = \"/content/drive/MyDrive/BLI_Data/data.csv\"\n",
        "df = pd.read_csv(PATH)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "i2zGrRGkJnWk",
        "outputId": "9bd15f17-f072-4f3b-8b6b-f521018ca619"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>#AUTHID</th>\n",
              "      <th>STATUS</th>\n",
              "      <th>sEXT</th>\n",
              "      <th>sNEU</th>\n",
              "      <th>sAGR</th>\n",
              "      <th>sCON</th>\n",
              "      <th>sOPN</th>\n",
              "      <th>cEXT</th>\n",
              "      <th>cNEU</th>\n",
              "      <th>cAGR</th>\n",
              "      <th>cCON</th>\n",
              "      <th>cOPN</th>\n",
              "      <th>DATE</th>\n",
              "      <th>NETWORKSIZE</th>\n",
              "      <th>BETWEENNESS</th>\n",
              "      <th>NBETWEENNESS</th>\n",
              "      <th>DENSITY</th>\n",
              "      <th>BROKERAGE</th>\n",
              "      <th>NBROKERAGE</th>\n",
              "      <th>TRANSITIVITY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5274</th>\n",
              "      <td>d39c2b0fb2e50e37795fdbe3b8cd3792</td>\n",
              "      <td>.......so apathetic........</td>\n",
              "      <td>3.75</td>\n",
              "      <td>2.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>4.25</td>\n",
              "      <td>4.50</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>10/13/09 07:16 PM</td>\n",
              "      <td>775.0</td>\n",
              "      <td>290973.00</td>\n",
              "      <td>97.27</td>\n",
              "      <td>0.01</td>\n",
              "      <td>296174.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7853</th>\n",
              "      <td>dba59aed04c6759a955a37f021e45bb7</td>\n",
              "      <td>doing a double shift at work.</td>\n",
              "      <td>1.75</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.05</td>\n",
              "      <td>3.05</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>09/11/09 04:23 AM</td>\n",
              "      <td>91.0</td>\n",
              "      <td>3763.98</td>\n",
              "      <td>93.98</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3899.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
              "      <td>likes the sound of thunder.</td>\n",
              "      <td>2.65</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.15</td>\n",
              "      <td>3.25</td>\n",
              "      <td>4.40</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>06/19/09 03:21 PM</td>\n",
              "      <td>180.0</td>\n",
              "      <td>14861.60</td>\n",
              "      <td>93.29</td>\n",
              "      <td>0.03</td>\n",
              "      <td>15661.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8816</th>\n",
              "      <td>b84d2613c4cf4e73f9c230f57facf66c</td>\n",
              "      <td>had to laugh when the doctor was cutting off h...</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>3.25</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.75</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>01/31/10 09:54 AM</td>\n",
              "      <td>243.0</td>\n",
              "      <td>27562.00</td>\n",
              "      <td>94.52</td>\n",
              "      <td>0.03</td>\n",
              "      <td>28498.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3149</th>\n",
              "      <td>b4a21c82de4011033c8ac67081ff939c</td>\n",
              "      <td>I think that *PROPNAME* was embarassed that we...</td>\n",
              "      <td>4.55</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.30</td>\n",
              "      <td>3.90</td>\n",
              "      <td>3.65</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>11/09/09 02:44 AM</td>\n",
              "      <td>254.0</td>\n",
              "      <td>30611.80</td>\n",
              "      <td>96.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>31359.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7088</th>\n",
              "      <td>540159466df2f050426692ddaac1eab4</td>\n",
              "      <td>doesn't like it when things don't go according...</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.75</td>\n",
              "      <td>4.00</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>12/15/09 01:39 AM</td>\n",
              "      <td>315.0</td>\n",
              "      <td>47078.60</td>\n",
              "      <td>95.80</td>\n",
              "      <td>0.03</td>\n",
              "      <td>48149.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4207</th>\n",
              "      <td>dba5f5266d03dd6d4db084ad7dbc683c</td>\n",
              "      <td>just tried kimchi and suchi! :D</td>\n",
              "      <td>1.55</td>\n",
              "      <td>2.45</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.40</td>\n",
              "      <td>4.50</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>10/11/09 05:03 AM</td>\n",
              "      <td>265.0</td>\n",
              "      <td>33752.40</td>\n",
              "      <td>97.22</td>\n",
              "      <td>0.02</td>\n",
              "      <td>34425.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6381</th>\n",
              "      <td>b576dc0449e75306c30456902ce0c80b</td>\n",
              "      <td>Random girl (In a bus) : Are you half chinese?...</td>\n",
              "      <td>3.84</td>\n",
              "      <td>2.60</td>\n",
              "      <td>3.75</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.15</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>09/25/09 07:09 AM</td>\n",
              "      <td>254.0</td>\n",
              "      <td>29724.90</td>\n",
              "      <td>93.25</td>\n",
              "      <td>0.04</td>\n",
              "      <td>30978.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7845</th>\n",
              "      <td>5880081cd3de1619cd431a75d9052dfc</td>\n",
              "      <td>Why is it that everyone else's musical tastes ...</td>\n",
              "      <td>2.90</td>\n",
              "      <td>2.20</td>\n",
              "      <td>3.05</td>\n",
              "      <td>3.90</td>\n",
              "      <td>4.10</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>10/06/09 02:08 AM</td>\n",
              "      <td>72.0</td>\n",
              "      <td>2015.24</td>\n",
              "      <td>81.10</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2262.0</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8666</th>\n",
              "      <td>7f3bfec0b7228d0900b01fbc8ce9d59f</td>\n",
              "      <td>\"... it's a very dangerous time, the coalition...</td>\n",
              "      <td>2.25</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.25</td>\n",
              "      <td>n</td>\n",
              "      <td>y</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>n</td>\n",
              "      <td>10/22/09 03:29 PM</td>\n",
              "      <td>1098.0</td>\n",
              "      <td>593081.00</td>\n",
              "      <td>98.66</td>\n",
              "      <td>0.01</td>\n",
              "      <td>598066.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               #AUTHID  ... TRANSITIVITY\n",
              "5274  d39c2b0fb2e50e37795fdbe3b8cd3792  ...         0.08\n",
              "7853  dba59aed04c6759a955a37f021e45bb7  ...         0.09\n",
              "0     b7b7764cfa1c523e4e93ab2a79a946c4  ...         0.10\n",
              "8816  b84d2613c4cf4e73f9c230f57facf66c  ...         0.15\n",
              "3149  b4a21c82de4011033c8ac67081ff939c  ...         0.09\n",
              "7088  540159466df2f050426692ddaac1eab4  ...         0.19\n",
              "4207  dba5f5266d03dd6d4db084ad7dbc683c  ...         0.03\n",
              "6381  b576dc0449e75306c30456902ce0c80b  ...         0.18\n",
              "7845  5880081cd3de1619cd431a75d9052dfc  ...         0.38\n",
              "8666  7f3bfec0b7228d0900b01fbc8ce9d59f  ...         0.04\n",
              "\n",
              "[10 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "yblTmwZsKKz3",
        "outputId": "9fa30843-a398-44a1-bab5-5a3cec482874"
      },
      "source": [
        "# Range of values where for each class being either positive or negative\n",
        "(pd.wide_to_long(df.reset_index(), i='index',\n",
        "                stubnames=['c','s'], j='cat',\n",
        "                suffix='.*' )\n",
        "   .groupby(['cat','c'])['s'].agg(['min','max'])\n",
        ")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <th>c</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">AGR</th>\n",
              "      <th>n</th>\n",
              "      <td>1.65</td>\n",
              "      <td>3.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>3.55</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">CON</th>\n",
              "      <th>n</th>\n",
              "      <td>1.45</td>\n",
              "      <td>3.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>3.50</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">EXT</th>\n",
              "      <th>n</th>\n",
              "      <td>1.33</td>\n",
              "      <td>3.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>3.60</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">NEU</th>\n",
              "      <th>n</th>\n",
              "      <td>1.25</td>\n",
              "      <td>2.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>2.80</td>\n",
              "      <td>4.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">OPN</th>\n",
              "      <th>n</th>\n",
              "      <td>2.25</td>\n",
              "      <td>3.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>3.80</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        min   max\n",
              "cat c            \n",
              "AGR n  1.65  3.50\n",
              "    y  3.55  5.00\n",
              "CON n  1.45  3.45\n",
              "    y  3.50  5.00\n",
              "EXT n  1.33  3.55\n",
              "    y  3.60  5.00\n",
              "NEU n  1.25  2.75\n",
              "    y  2.80  4.75\n",
              "OPN n  2.25  3.75\n",
              "    y  3.80  5.00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdeurXAbRlFO"
      },
      "source": [
        "cols = [\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]\n",
        "\n",
        "df[cols] = df[cols].replace({\"n\" : 0, \"y\" : 1})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fVp7UkRL6tz"
      },
      "source": [
        "cols_ = [\"STATUS\",\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]\n",
        "df = df[cols_]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jmW2D9W8MmaP",
        "outputId": "08c87b6c-3fd5-4875-d568-b7e0b5223a48"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STATUS</th>\n",
              "      <th>cEXT</th>\n",
              "      <th>cNEU</th>\n",
              "      <th>cAGR</th>\n",
              "      <th>cCON</th>\n",
              "      <th>cOPN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>likes the sound of thunder.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is sore and wants the knot of muscles at the b...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>likes how the day sounds in this new song.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is home. &lt;3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              STATUS  cEXT  ...  cCON  cOPN\n",
              "0                        likes the sound of thunder.     0  ...     0     1\n",
              "1  is so sleepy it's not even funny that's she ca...     0  ...     0     1\n",
              "2  is sore and wants the knot of muscles at the b...     0  ...     0     1\n",
              "3         likes how the day sounds in this new song.     0  ...     0     1\n",
              "4                                        is home. <3     0  ...     0     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMpkN20uccgh"
      },
      "source": [
        "def sub_preprocess(sub):\n",
        "  # run regex to remove certain characters\n",
        "  sub['STATUS'] = sub['STATUS'].map(lambda x: re.sub(r\"[@\\?\\.$%_\\[\\]()+-:*\\\"]\", ' ', x, flags=re.I))\n",
        "  sub['STATUS'] = sub['STATUS'].map(lambda x: re.sub(r\"[,']\", '', x, flags=re.I))\n",
        "  sub['STATUS'] = sub['STATUS'].map(lambda x: re.sub(\"(?<![\\w'])\\w+?(?=\\b|'s)\", ' ', x))# run regex to remove line breaks and tabs\n",
        "  sub['STATUS'] = sub['STATUS'].map(lambda x: re.sub(r\"\\s+\", ' ', x))\n",
        "\n",
        "sub_preprocess(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-u719P-ce6-"
      },
      "source": [
        "import string\n",
        "\n",
        "def clean_text_round1(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "df['STATUS'] = df['STATUS'].apply(lambda x: clean_text_round1(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4OTjEvIcfbv"
      },
      "source": [
        "def clean_text_round2(text):\n",
        "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "\n",
        "df['STATUS'] = df['STATUS'].apply(lambda x: clean_text_round2(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "VoSmHrIJdQiA",
        "outputId": "68801493-93ba-4481-ae45-8df9e611cf90"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STATUS</th>\n",
              "      <th>cEXT</th>\n",
              "      <th>cNEU</th>\n",
              "      <th>cAGR</th>\n",
              "      <th>cCON</th>\n",
              "      <th>cOPN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8511</th>\n",
              "      <td>likes food.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2895</th>\n",
              "      <td>better ace my exam considering i missed the be...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>Dear Baby Jesus: Please compell my boyfriend t...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7986</th>\n",
              "      <td>Ladies and gentlemen, we are floating in space...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9758</th>\n",
              "      <td>Go Gators! 8 and 0.... Now, lets see if the Vo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6352</th>\n",
              "      <td>My new froggies are so cute....names are *PROP...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1368</th>\n",
              "      <td>~ grr... school sucks.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>saw the official end of the New England Era.  ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3095</th>\n",
              "      <td>is not with his darling anymore :( sad day(s)....</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9590</th>\n",
              "      <td>didn't get home until 5:30am ... cause she had...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 STATUS  cEXT  ...  cCON  cOPN\n",
              "8511                                        likes food.     1  ...     1     1\n",
              "2895  better ace my exam considering i missed the be...     1  ...     1     1\n",
              "3196  Dear Baby Jesus: Please compell my boyfriend t...     1  ...     1     0\n",
              "7986  Ladies and gentlemen, we are floating in space...     0  ...     0     0\n",
              "9758  Go Gators! 8 and 0.... Now, lets see if the Vo...     0  ...     0     1\n",
              "6352  My new froggies are so cute....names are *PROP...     1  ...     0     0\n",
              "1368                             ~ grr... school sucks.     0  ...     1     1\n",
              "692   saw the official end of the New England Era.  ...     0  ...     0     1\n",
              "3095  is not with his darling anymore :( sad day(s)....     1  ...     1     1\n",
              "9590  didn't get home until 5:30am ... cause she had...     1  ...     1     1\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT6cKjsImH1E"
      },
      "source": [
        "df['lists'] = df[df.columns[1:6]].values.tolist()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmYWjb9pmNXm"
      },
      "source": [
        "df = df[['STATUS','lists']]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "M-DQchE8mc4j",
        "outputId": "aed62b6a-1d06-40e2-d010-34f34041198c"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STATUS</th>\n",
              "      <th>lists</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>Dear Baby Jesus: Please compell my boyfriend t...</td>\n",
              "      <td>[1, 0, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5684</th>\n",
              "      <td>is missing you.</td>\n",
              "      <td>[0, 0, 1, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>and the line between yin and yang.</td>\n",
              "      <td>[0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1162</th>\n",
              "      <td>is excited for the newest movie adaptation of ...</td>\n",
              "      <td>[0, 0, 1, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8587</th>\n",
              "      <td>Santa isn't bringing me presents this year. He...</td>\n",
              "      <td>[0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>is always surprised at how many building mater...</td>\n",
              "      <td>[0, 0, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9551</th>\n",
              "      <td>coastin......;-)</td>\n",
              "      <td>[1, 0, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>got sick of the crap �5 haircuts so upgraded t...</td>\n",
              "      <td>[0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5299</th>\n",
              "      <td>thinks she may have to walk to class in the mo...</td>\n",
              "      <td>[1, 0, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4660</th>\n",
              "      <td>work, work, work until 6. Day-off tomorrow to ...</td>\n",
              "      <td>[1, 0, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 STATUS            lists\n",
              "3196  Dear Baby Jesus: Please compell my boyfriend t...  [1, 0, 0, 1, 0]\n",
              "5684                                    is missing you.  [0, 0, 1, 0, 1]\n",
              "649                  and the line between yin and yang.  [0, 0, 0, 0, 1]\n",
              "1162  is excited for the newest movie adaptation of ...  [0, 0, 1, 0, 1]\n",
              "8587  Santa isn't bringing me presents this year. He...  [0, 0, 0, 0, 0]\n",
              "2617  is always surprised at how many building mater...  [0, 0, 1, 1, 1]\n",
              "9551                                   coastin......;-)  [1, 0, 1, 1, 1]\n",
              "1113  got sick of the crap �5 haircuts so upgraded t...  [0, 0, 0, 0, 0]\n",
              "5299  thinks she may have to walk to class in the mo...  [1, 0, 1, 1, 1]\n",
              "4660  work, work, work until 6. Day-off tomorrow to ...  [1, 0, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R743xwejn9-J"
      },
      "source": [
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-5\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6x3lHvNSQxK"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42 #so our results/process is reproducible by whoever wants to reproduce\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val) #include for when using a GPU"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHep39N3ZLsd"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.STATUS = dataframe.STATUS\n",
        "        self.targets = self.data.lists\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.STATUS)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        STATUS = str(self.STATUS[index])\n",
        "        STATUS = \" \".join(STATUS.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            STATUS,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcnBlNR4n0-f",
        "outputId": "5ec4eaf4-adc9-419e-bb2f-c350e5bc88b6"
      },
      "source": [
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=seed_val)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (9917, 2)\n",
            "TRAIN Dataset: (7934, 2)\n",
            "TEST Dataset: (1983, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSE9Wp18n3A0"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 4\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 1\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_efc3tf-oDPA",
        "outputId": "b170b5c8-f104-404f-d35e-817f250bae93"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 5)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn9fH092oPZ9"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVATcJ-tpBY-"
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler, epoch):\n",
        "  # Put the model in training mode\n",
        "  model.train()\n",
        "\n",
        "  # loop over all the batches\n",
        "  for i, d in enumerate(data_loader):\n",
        "    ids = d[\"ids\"]\n",
        "    token_type_ids = d[\"token_type_ids\"]\n",
        "    mask = d[\"mask\"]\n",
        "    targets = d[\"targets\"]\n",
        "\n",
        "    # convert to device type\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "    # zero-grad the optimizer\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(\n",
        "        ids=ids,\n",
        "        mask=mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "    # calculate the loss\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    if i % 5000==0:\n",
        "      print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "    # backward step the loss\n",
        "    loss.backward()\n",
        "    # step the optimizer\n",
        "    optimizer.step()\n",
        "    # step scheduler\n",
        "    scheduler.step()\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thK0ZVeYEDBx"
      },
      "source": [
        "def eval_fn(data_loader, model, device):\n",
        "  # put the model in eval mode\n",
        "  model.eval()\n",
        "  # initialize empty lists for targets and outputs\n",
        "  fin_targets = []\n",
        "  fin_outputs = []\n",
        "  \n",
        "  # no_grad to prevent unnecessary use of GPU memory\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      ids = d[\"ids\"]\n",
        "      token_type_ids = d[\"token_type_ids\"]\n",
        "      mask = d[\"mask\"]\n",
        "      targets = d[\"targets\"]\n",
        "  \n",
        "      # convert to device type\n",
        "      ids = ids.to(device, dtype=torch.long)\n",
        "      token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "      mask = mask.to(device, dtype=torch.long)\n",
        "      targets = targets.to(device, dtype=torch.float)\n",
        "  \n",
        "      outputs = model(\n",
        "          ids=ids,\n",
        "          mask=mask,\n",
        "          token_type_ids=token_type_ids\n",
        "      )\n",
        "  \n",
        "      # convert targets to cpu and extend the final list\n",
        "      targets = targets.cpu().detach()\n",
        "      fin_targets.extend(targets.numpy().tolist())\n",
        "      outputs = torch.sigmoid(outputs).cpu().detach()\n",
        "      fin_outputs.extend(outputs.numpy().tolist())\n",
        "  return fin_outputs, fin_targets\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8-0DQb-oPv_"
      },
      "source": [
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE, #2e-5 > 5e-5: A HYPERPARAMETER\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(training_loader) * EPOCHS\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLRSfWMQGeGh",
        "outputId": "3a217f91-96bd-4802-b0b6-6c8858ed3fba"
      },
      "source": [
        "best_f1_score = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  train_fn(training_loader,model, optimizer, device, scheduler, epoch)\n",
        "  outputs, targets = eval_fn(testing_loader, model, device)\n",
        "  outputs = np.array(outputs) >= 0.5\n",
        "  accuracy = accuracy_score(targets,outputs)\n",
        "  print(f\"Accuracy score = {accuracy}\")\n",
        "  f1_score_micro = f1_score(targets, outputs, average='micro')\n",
        "  f1_score_macro = f1_score(targets, outputs, average='macro')\n",
        "  confusion_matrix = multilabel_confusion_matrix(targets, outputs)\n",
        "  print(f\"Accuracy Score = {accuracy}\")\n",
        "  print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "  print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
        "  print(classification_report(targets, outputs, zero_division=0))\n",
        "  if f1_score_micro > best_f1_score:\n",
        "    torch.save(model.state_dict(), f\"/content/drive/MyDrive/BLI_Data/model-epoch-{epoch}.bin\")\n",
        "    best_f1_score = f1_score_micro"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.7076229453086853\n",
            "Accuracy score = 0.10993444276348967\n",
            "Accuracy Score = 0.10993444276348967\n",
            "F1 Score (Micro) = 0.6192307692307693\n",
            "F1 Score (Macro) = 0.4967605681463379\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.39      0.44       832\n",
            "           1       0.00      0.00      0.00       733\n",
            "           2       0.59      0.67      0.63      1049\n",
            "           3       0.51      0.63      0.56       880\n",
            "           4       0.75      1.00      0.85      1478\n",
            "\n",
            "   micro avg       0.62      0.62      0.62      4972\n",
            "   macro avg       0.47      0.54      0.50      4972\n",
            "weighted avg       0.52      0.62      0.56      4972\n",
            " samples avg       0.66      0.60      0.57      4972\n",
            "\n",
            "Epoch: 1, Loss:  0.6534436345100403\n",
            "Accuracy score = 0.1356530509329299\n",
            "Accuracy Score = 0.1356530509329299\n",
            "F1 Score (Micro) = 0.6271257610749528\n",
            "F1 Score (Macro) = 0.5368998573204807\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.30      0.39       832\n",
            "           1       0.54      0.18      0.27       733\n",
            "           2       0.60      0.66      0.63      1049\n",
            "           3       0.58      0.50      0.53       880\n",
            "           4       0.76      1.00      0.86      1478\n",
            "\n",
            "   micro avg       0.66      0.60      0.63      4972\n",
            "   macro avg       0.61      0.53      0.54      4972\n",
            "weighted avg       0.63      0.60      0.59      4972\n",
            " samples avg       0.67      0.59      0.59      4972\n",
            "\n",
            "Epoch: 2, Loss:  0.6115862727165222\n",
            "Accuracy score = 0.156833081190116\n",
            "Accuracy Score = 0.156833081190116\n",
            "F1 Score (Micro) = 0.6440610352049466\n",
            "F1 Score (Macro) = 0.5892828184621145\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.50      0.51       832\n",
            "           1       0.51      0.35      0.41       733\n",
            "           2       0.61      0.68      0.64      1049\n",
            "           3       0.58      0.48      0.52       880\n",
            "           4       0.77      0.96      0.86      1478\n",
            "\n",
            "   micro avg       0.64      0.65      0.64      4972\n",
            "   macro avg       0.60      0.59      0.59      4972\n",
            "weighted avg       0.62      0.65      0.63      4972\n",
            " samples avg       0.64      0.64      0.61      4972\n",
            "\n",
            "Epoch: 3, Loss:  0.6003116965293884\n",
            "Accuracy score = 0.18305597579425115\n",
            "Accuracy Score = 0.18305597579425115\n",
            "F1 Score (Micro) = 0.6438037430450176\n",
            "F1 Score (Macro) = 0.5987092664262488\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.53      0.54       832\n",
            "           1       0.54      0.36      0.43       733\n",
            "           2       0.63      0.64      0.64      1049\n",
            "           3       0.56      0.55      0.56       880\n",
            "           4       0.79      0.89      0.84      1478\n",
            "\n",
            "   micro avg       0.65      0.64      0.64      4972\n",
            "   macro avg       0.61      0.59      0.60      4972\n",
            "weighted avg       0.64      0.64      0.63      4972\n",
            " samples avg       0.64      0.64      0.60      4972\n",
            "\n",
            "Epoch: 4, Loss:  0.335233598947525\n",
            "Accuracy score = 0.17750882501260717\n",
            "Accuracy Score = 0.17750882501260717\n",
            "F1 Score (Micro) = 0.6476436667329489\n",
            "F1 Score (Macro) = 0.6090452171627564\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.45      0.51       832\n",
            "           1       0.52      0.45      0.49       733\n",
            "           2       0.61      0.70      0.65      1049\n",
            "           3       0.54      0.60      0.57       880\n",
            "           4       0.79      0.87      0.83      1478\n",
            "\n",
            "   micro avg       0.64      0.66      0.65      4972\n",
            "   macro avg       0.61      0.62      0.61      4972\n",
            "weighted avg       0.63      0.66      0.64      4972\n",
            " samples avg       0.64      0.65      0.61      4972\n",
            "\n",
            "Epoch: 5, Loss:  0.34832972288131714\n",
            "Accuracy score = 0.17750882501260717\n",
            "Accuracy Score = 0.17750882501260717\n",
            "F1 Score (Micro) = 0.6643424365678868\n",
            "F1 Score (Macro) = 0.6197306028847989\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.59      0.57       832\n",
            "           1       0.53      0.39      0.45       733\n",
            "           2       0.60      0.76      0.67      1049\n",
            "           3       0.58      0.54      0.56       880\n",
            "           4       0.78      0.93      0.85      1478\n",
            "\n",
            "   micro avg       0.64      0.69      0.66      4972\n",
            "   macro avg       0.61      0.64      0.62      4972\n",
            "weighted avg       0.63      0.69      0.65      4972\n",
            " samples avg       0.65      0.69      0.63      4972\n",
            "\n",
            "Epoch: 6, Loss:  0.4043302536010742\n",
            "Accuracy score = 0.18658598083711547\n",
            "Accuracy Score = 0.18658598083711547\n",
            "F1 Score (Micro) = 0.6565200314218382\n",
            "F1 Score (Macro) = 0.6219893609409048\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.51      0.53       832\n",
            "           1       0.53      0.47      0.50       733\n",
            "           2       0.62      0.69      0.66      1049\n",
            "           3       0.55      0.64      0.59       880\n",
            "           4       0.80      0.87      0.83      1478\n",
            "\n",
            "   micro avg       0.64      0.67      0.66      4972\n",
            "   macro avg       0.61      0.64      0.62      4972\n",
            "weighted avg       0.64      0.67      0.65      4972\n",
            " samples avg       0.64      0.67      0.63      4972\n",
            "\n",
            "Epoch: 7, Loss:  0.21855854988098145\n",
            "Accuracy score = 0.19213313161875945\n",
            "Accuracy Score = 0.19213313161875945\n",
            "F1 Score (Micro) = 0.6527105756887528\n",
            "F1 Score (Macro) = 0.6207463595431826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.58      0.56       832\n",
            "           1       0.52      0.47      0.50       733\n",
            "           2       0.63      0.65      0.64      1049\n",
            "           3       0.56      0.60      0.58       880\n",
            "           4       0.80      0.86      0.83      1478\n",
            "\n",
            "   micro avg       0.64      0.66      0.65      4972\n",
            "   macro avg       0.61      0.63      0.62      4972\n",
            "weighted avg       0.64      0.66      0.65      4972\n",
            " samples avg       0.64      0.67      0.62      4972\n",
            "\n",
            "Epoch: 8, Loss:  0.22137418389320374\n",
            "Accuracy score = 0.19213313161875945\n",
            "Accuracy Score = 0.19213313161875945\n",
            "F1 Score (Micro) = 0.652604476133888\n",
            "F1 Score (Macro) = 0.6158680520126555\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.58      0.55       832\n",
            "           1       0.57      0.39      0.46       733\n",
            "           2       0.62      0.70      0.66      1049\n",
            "           3       0.56      0.61      0.58       880\n",
            "           4       0.80      0.85      0.83      1478\n",
            "\n",
            "   micro avg       0.64      0.66      0.65      4972\n",
            "   macro avg       0.62      0.63      0.62      4972\n",
            "weighted avg       0.64      0.66      0.65      4972\n",
            " samples avg       0.64      0.65      0.62      4972\n",
            "\n",
            "Epoch: 9, Loss:  0.17411582171916962\n",
            "Accuracy score = 0.18910741301059\n",
            "Accuracy Score = 0.18910741301059\n",
            "F1 Score (Micro) = 0.64887271256698\n",
            "F1 Score (Macro) = 0.6067679719733956\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55       832\n",
            "           1       0.58      0.34      0.43       733\n",
            "           2       0.63      0.68      0.65      1049\n",
            "           3       0.56      0.59      0.57       880\n",
            "           4       0.81      0.85      0.83      1478\n",
            "\n",
            "   micro avg       0.65      0.65      0.65      4972\n",
            "   macro avg       0.62      0.60      0.61      4972\n",
            "weighted avg       0.65      0.65      0.64      4972\n",
            " samples avg       0.65      0.64      0.61      4972\n",
            "\n",
            "Epoch: 10, Loss:  0.09597527980804443\n",
            "Accuracy score = 0.17650025214321735\n",
            "Accuracy Score = 0.17650025214321735\n",
            "F1 Score (Micro) = 0.6606326034063259\n",
            "F1 Score (Macro) = 0.6229761312682507\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.57      0.56       832\n",
            "           1       0.55      0.41      0.47       733\n",
            "           2       0.61      0.76      0.68      1049\n",
            "           3       0.55      0.60      0.58       880\n",
            "           4       0.80      0.87      0.83      1478\n",
            "\n",
            "   micro avg       0.64      0.68      0.66      4972\n",
            "   macro avg       0.61      0.64      0.62      4972\n",
            "weighted avg       0.64      0.68      0.66      4972\n",
            " samples avg       0.64      0.68      0.63      4972\n",
            "\n",
            "Epoch: 11, Loss:  0.06181136518716812\n",
            "Accuracy score = 0.18356026222894603\n",
            "Accuracy Score = 0.18356026222894603\n",
            "F1 Score (Micro) = 0.6620303293731287\n",
            "F1 Score (Macro) = 0.6262654802498739\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55       832\n",
            "           1       0.55      0.43      0.48       733\n",
            "           2       0.62      0.72      0.67      1049\n",
            "           3       0.54      0.66      0.59       880\n",
            "           4       0.80      0.88      0.84      1478\n",
            "\n",
            "   micro avg       0.64      0.69      0.66      4972\n",
            "   macro avg       0.61      0.65      0.63      4972\n",
            "weighted avg       0.63      0.69      0.66      4972\n",
            " samples avg       0.65      0.68      0.63      4972\n",
            "\n",
            "Epoch: 12, Loss:  0.10171794146299362\n",
            "Accuracy score = 0.18608169440242056\n",
            "Accuracy Score = 0.18608169440242056\n",
            "F1 Score (Micro) = 0.6675007217784621\n",
            "F1 Score (Macro) = 0.6305569250103114\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.57      0.55       832\n",
            "           1       0.56      0.46      0.50       733\n",
            "           2       0.62      0.69      0.65      1049\n",
            "           3       0.56      0.64      0.60       880\n",
            "           4       0.78      0.93      0.85      1478\n",
            "\n",
            "   micro avg       0.64      0.70      0.67      4972\n",
            "   macro avg       0.61      0.66      0.63      4972\n",
            "weighted avg       0.63      0.70      0.66      4972\n",
            " samples avg       0.65      0.70      0.64      4972\n",
            "\n",
            "Epoch: 13, Loss:  0.05277562141418457\n",
            "Accuracy score = 0.18356026222894603\n",
            "Accuracy Score = 0.18356026222894603\n",
            "F1 Score (Micro) = 0.6567282061407839\n",
            "F1 Score (Macro) = 0.6143183610151501\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.58      0.55       832\n",
            "           1       0.59      0.35      0.44       733\n",
            "           2       0.62      0.70      0.66      1049\n",
            "           3       0.55      0.61      0.58       880\n",
            "           4       0.80      0.89      0.84      1478\n",
            "\n",
            "   micro avg       0.64      0.67      0.66      4972\n",
            "   macro avg       0.62      0.63      0.61      4972\n",
            "weighted avg       0.64      0.67      0.65      4972\n",
            " samples avg       0.65      0.66      0.62      4972\n",
            "\n",
            "Epoch: 14, Loss:  0.1478729397058487\n",
            "Accuracy score = 0.18406454866364094\n",
            "Accuracy Score = 0.18406454866364094\n",
            "F1 Score (Micro) = 0.6560047095761382\n",
            "F1 Score (Macro) = 0.6198549500071451\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.56      0.54       832\n",
            "           1       0.54      0.44      0.48       733\n",
            "           2       0.62      0.67      0.64      1049\n",
            "           3       0.55      0.63      0.59       880\n",
            "           4       0.80      0.88      0.84      1478\n",
            "\n",
            "   micro avg       0.64      0.67      0.66      4972\n",
            "   macro avg       0.61      0.63      0.62      4972\n",
            "weighted avg       0.64      0.67      0.65      4972\n",
            " samples avg       0.65      0.67      0.62      4972\n",
            "\n",
            "Epoch: 15, Loss:  0.060543205589056015\n",
            "Accuracy score = 0.19314170448814927\n",
            "Accuracy Score = 0.19314170448814927\n",
            "F1 Score (Micro) = 0.6564764267990074\n",
            "F1 Score (Macro) = 0.6182860970178985\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.59      0.56       832\n",
            "           1       0.57      0.40      0.47       733\n",
            "           2       0.62      0.67      0.65      1049\n",
            "           3       0.57      0.60      0.58       880\n",
            "           4       0.80      0.88      0.84      1478\n",
            "\n",
            "   micro avg       0.65      0.67      0.66      4972\n",
            "   macro avg       0.62      0.63      0.62      4972\n",
            "weighted avg       0.64      0.67      0.65      4972\n",
            " samples avg       0.65      0.66      0.62      4972\n",
            "\n",
            "Epoch: 16, Loss:  0.045279279351234436\n",
            "Accuracy score = 0.18255168935955624\n",
            "Accuracy Score = 0.18255168935955624\n",
            "F1 Score (Micro) = 0.6577935522034902\n",
            "F1 Score (Macro) = 0.6190231074931687\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.54      0.55       832\n",
            "           1       0.55      0.42      0.48       733\n",
            "           2       0.62      0.69      0.65      1049\n",
            "           3       0.55      0.60      0.58       880\n",
            "           4       0.79      0.89      0.84      1478\n",
            "\n",
            "   micro avg       0.65      0.67      0.66      4972\n",
            "   macro avg       0.61      0.63      0.62      4972\n",
            "weighted avg       0.64      0.67      0.65      4972\n",
            " samples avg       0.65      0.67      0.63      4972\n",
            "\n",
            "Epoch: 17, Loss:  0.0985381230711937\n",
            "Accuracy score = 0.1880988401412002\n",
            "Accuracy Score = 0.1880988401412002\n",
            "F1 Score (Micro) = 0.6555489378598371\n",
            "F1 Score (Macro) = 0.6185471739467229\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55       832\n",
            "           1       0.56      0.41      0.47       733\n",
            "           2       0.63      0.67      0.65      1049\n",
            "           3       0.55      0.62      0.59       880\n",
            "           4       0.80      0.87      0.84      1478\n",
            "\n",
            "   micro avg       0.65      0.66      0.66      4972\n",
            "   macro avg       0.62      0.63      0.62      4972\n",
            "weighted avg       0.64      0.66      0.65      4972\n",
            " samples avg       0.65      0.66      0.62      4972\n",
            "\n",
            "Epoch: 18, Loss:  0.013946997001767159\n",
            "Accuracy score = 0.1875945537065053\n",
            "Accuracy Score = 0.1875945537065053\n",
            "F1 Score (Micro) = 0.6573067035791971\n",
            "F1 Score (Macro) = 0.6206133049775585\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.58      0.56       832\n",
            "           1       0.57      0.41      0.47       733\n",
            "           2       0.62      0.69      0.65      1049\n",
            "           3       0.56      0.61      0.58       880\n",
            "           4       0.80      0.87      0.83      1478\n",
            "\n",
            "   micro avg       0.65      0.67      0.66      4972\n",
            "   macro avg       0.62      0.63      0.62      4972\n",
            "weighted avg       0.64      0.67      0.65      4972\n",
            " samples avg       0.65      0.66      0.62      4972\n",
            "\n",
            "Epoch: 19, Loss:  0.01732155680656433\n",
            "Accuracy score = 0.18709026727181038\n",
            "Accuracy Score = 0.18709026727181038\n",
            "F1 Score (Micro) = 0.6586590528699419\n",
            "F1 Score (Macro) = 0.6222167308361545\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.57      0.56       832\n",
            "           1       0.56      0.42      0.48       733\n",
            "           2       0.62      0.70      0.66      1049\n",
            "           3       0.56      0.61      0.58       880\n",
            "           4       0.80      0.87      0.83      1478\n",
            "\n",
            "   micro avg       0.65      0.67      0.66      4972\n",
            "   macro avg       0.62      0.63      0.62      4972\n",
            "weighted avg       0.64      0.67      0.65      4972\n",
            " samples avg       0.65      0.67      0.63      4972\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYqiiLlcMVo4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}